{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushmanace/Computer-Vision/blob/main/Artificial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_y4S6RYbV00O"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m6vEgHA0xRXA"
      },
      "outputs": [],
      "source": [
        "#!pip install pygad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XRw2kQx_cfEo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WSyCKlRpUuVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d5ad5f-2f61-4889-83e1-18e2989a3820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WhEzx-28VWLe"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "54TpxS-GUwke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e76f32c-7683-4c0e-e082-c28bb08c02d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Main_Dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "r = os.listdir(path)\n",
        "r[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nd_8ahubU24F"
      },
      "outputs": [],
      "source": [
        "csv_files = glob.glob(os.path.join(path, \"Main_dataset\")+\"/*.csv\")\n",
        "\n",
        "for file in csv_files:\n",
        "    # Get the filename without the path or extension\n",
        "    filename = os.path.splitext(os.path.basename(file))[0]\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame with the same name as the file\n",
        "    globals()[filename] = pd.read_csv(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FnjIkMRgZS6G"
      },
      "outputs": [],
      "source": [
        "path2 = \"/content/drive/MyDrive/Dataset/Prepared_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0JXFcbIWZe6U"
      },
      "outputs": [],
      "source": [
        "data_dir = path2\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "val_dir = os.path.join(data_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3A-IPh1V7QG"
      },
      "outputs": [],
      "source": [
        "# Define the image and mask directories\n",
        "image_dir = 'images'\n",
        "mask_dir = 'masks'\n",
        "\n",
        "# Define a function to get the file paths for all the images and masks in a directory\n",
        "def get_image_mask_paths(dir_path):\n",
        "    # Get the paths for all the image files\n",
        "    image_paths = [os.path.join(dir_path, image_dir, f) for f in os.listdir(os.path.join(dir_path, image_dir))]\n",
        "    \n",
        "    # Get the corresponding paths for the mask files\n",
        "    mask_paths = [os.path.join(dir_path, mask_dir, os.path.splitext(f)[0] + '.png') for f in os.listdir(os.path.join(dir_path, image_dir)) ]\n",
        "\n",
        "\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "# Get the file paths for all the images and masks in the train, test, and val directories\n",
        "train_image_paths, train_mask_paths = get_image_mask_paths(train_dir)\n",
        "test_image_paths, test_mask_paths  = get_image_mask_paths(test_dir)\n",
        "val_image_paths, val_mask_paths = get_image_mask_paths(val_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uolj5RADbM0b"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths,transform = None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "         # Convert PIL images to PyTorch tensors\n",
        "        x = torch.Tensor(np.array(image))\n",
        "        image = (x - x.min()) / (x.max() - x.min())\n",
        "        y = torch.Tensor(np.array(mask))\n",
        "        mask = (y - y.min())/(y.max() - y.min())\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_-nuvcJbYHn"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_image_paths[:1024], train_mask_paths[:1024])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "val_dataset = CustomDataset(val_image_paths[:24], val_mask_paths[:24])\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH3WAbL5bE1z"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda import amp\n",
        "\n",
        "\n",
        "##Define the neural network \n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv,self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=3, features =[64, 128, 256, 512]):\n",
        "    super(UNet, self).__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    #down part of UNet\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    #upsample part of UNet\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(\n",
        "          nn.ConvTranspose2d(\n",
        "              feature*2, feature, kernel_size=2, stride = 2\n",
        "          )\n",
        "      )\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections = skip_connections[::-1]\n",
        "\n",
        "    for idx in range(0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = TF.resize(x, size = skip_connection.shape[2:])\n",
        "        \n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "      x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "    return self.final_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNVByGqSbHgH"
      },
      "outputs": [],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "model =  UNet()\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu"
      ],
      "metadata": {
        "id": "4BHXAH_eKNBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--8j1Cgxetmj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def images_to_array(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = Image.open(path)\n",
        "        img = np.array(img)\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "data_inputs = images_to_array(train_image_paths[:128])\n",
        "data_outputs = images_to_array(train_mask_paths[:128])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7oD_VP-ZowB"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "\"\"\"\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "pop_size = 10\n",
        "num_generations = 5\n",
        "num_parents_mating = 5\n",
        "\n",
        "def callback_generation(ga_instance):\n",
        "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
        "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
        "\n",
        "def fitness_func(solution, sol_idx):\n",
        "    global data_inputs, data_outputs, torch_ga, model, loss_function\n",
        "\n",
        "    predictions = pygad.torchga.predict(model=model, \n",
        "                                        solution=solution, \n",
        "                                        data=data_inputs)\n",
        "\n",
        "    solution_fitness = 1.0 / (loss_function(predictions, data_outputs).detach().numpy() + 0.00000001)\n",
        "\n",
        "    return solution_fitness\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "torch_ga = pygad.torchga.TorchGA(model=model,num_solutions=10)\n",
        "\n",
        "\n",
        "initial_population = torch_ga.population_weights # Initial population of network weights.\n",
        "\n",
        "data_inputs = torch.from_numpy(data_inputs).float()\n",
        "data_outputs = torch.from_numpy(data_outputs).float()\n",
        "\n",
        "data_inputs = data_inputs.permute(0,3,1,2)\n",
        "data_outputs = data_outputs.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "# Create an instance of the pygad.GA class\n",
        "ga_instance = pygad.GA(num_generations=num_generations, \n",
        "                       num_parents_mating=num_parents_mating, \n",
        "                       initial_population=initial_population,\n",
        "                       fitness_func=fitness_func,\n",
        "                       on_generation=callback_generation)\n",
        "\n",
        "# Start the genetic algorithm evolution.\n",
        "ga_instance.run()\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGz7iShcbVLE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epoch = 100\n",
        "\n",
        "print(\"Training Begins\")\n",
        "for epoch in range(num_epoch):\n",
        "  running_loss = 0.0\n",
        "  for batch_idx,(data, target) in enumerate(train_loader):\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model.forward(data.permute(0,3,1,2))\n",
        "\n",
        "    loss = criterion(output, target.permute(0,3,1,2))\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  if epoch%10==0 or epoch==num_epoch-1:\n",
        "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epoch, running_loss))\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            if train_on_gpu:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            outputs = model(inputs.permute(0,3,1,2))\n",
        "            loss = criterion(outputs, labels.permute(0,3,1,2))\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            #_, preds = torch.max(outputs, 1)\n",
        "            #val_acc += torch.sum(preds == labels.data)\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    #val_acc = val_acc / len(val_loader.dataset)\n",
        "    print('Validation Loss: {:.4f}'.format(val_loss))\n",
        "\n",
        "validate(model, val_loader, criterion )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jChpYdS35ony"
      },
      "source": [
        "## Testing the Model Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIoivkfObkC4"
      },
      "outputs": [],
      "source": [
        "iterator = iter(val_loader)\n",
        "sample = next(iterator)\n",
        "a,b = sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2N6wrtKqMwNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jgA0AdAe2eO"
      },
      "outputs": [],
      "source": [
        "a = a.cuda()\n",
        "c = model(a.permute(0,3,1,2))\n",
        "t = c.permute(0,2,3,1)\n",
        "t  = t.cpu()\n",
        "t = t.detach().numpy()\n",
        "a = a.cpu()\n",
        "a = a.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-keb6Q84OHW"
      },
      "outputs": [],
      "source": [
        "t.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiEPduhK4yt1"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=8, ncols=3,figsize=(10,20))\n",
        "for i in range(t.shape[0]):\n",
        "  ax[i, 0].imshow(np.clip(a[i], 0, 1))\n",
        "  ax[i, 0].axis('off')\n",
        "  ax[i, 1].imshow(np.clip(t[i], 0, 1))\n",
        "  ax[i, 1].axis('off')\n",
        "  ax[i, 2].imshow(np.clip(b[i], 0, 1))\n",
        "  ax[i, 2].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRLokNVz48F0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba47da2YkspN"
      },
      "outputs": [],
      "source": [
        "for data, target in val_loader:\n",
        "  print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SFcNiD4kuKz"
      },
      "outputs": [],
      "source": [
        "for data, target in val_loader:\n",
        "  print(data.permute(0,3,1,2).shape)\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  model.forward(data.permute(0,3,1,2)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCchVMkPk-JK"
      },
      "outputs": [],
      "source": [
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-QvdGBBmMqE"
      },
      "outputs": [],
      "source": [
        "model.forward(a.permute(0,3,1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF9xEiIbmpsy"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(8,3,120,120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1jBWBzGyiYV"
      },
      "outputs": [],
      "source": [
        "a.permute(0,3,2,1).permute(0,3,1,2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c05zhwVy7mU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA4EYdn++D5fsFCphjqWQF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}